{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = []\n",
    "sent_words = []\n",
    "with open('../1998-01-2003版-带音.txt', 'r', encoding='gbk') as f:\n",
    "    for line in f.readlines():\n",
    "        sent = ''\n",
    "        sent_word = []\n",
    "        words = line.strip().split('  ')[1:]\n",
    "        for word in words:\n",
    "            sent += (word.split('/')[0].strip() + ' ')\n",
    "            sent_word.append(word.split('/')[0].strip())\n",
    "        sents.append(sent)\n",
    "        sent_words.append(sent_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_set = []\n",
    "for sent_word in sent_words:\n",
    "    words_set += ''.join(sent_word)\n",
    "all_words = words_set\n",
    "words_set = list(set(words_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./unigram.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(dict(collections.Counter(all_words))))\n",
    "with open('./bigram.json', 'w', encoding='utf-8') as f:\n",
    "    bigrams = []\n",
    "    for word1, word2 in zip(all_words[:-1], all_words[1:]):\n",
    "        bigrams.append(word1+word2)\n",
    "    f.write(json.dumps(dict(collections.Counter(bigrams))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marked_sent_words = []\n",
    "for sent_word in sent_words:\n",
    "    marks = []\n",
    "    for word in sent_word:\n",
    "        if len(word) == 1:\n",
    "            marks.append('s')\n",
    "        elif len(word) == 2:\n",
    "            marks.append('be')\n",
    "        elif len(word) > 2:\n",
    "            marks.append('b'+('m'*(len(word)-2))+'e')\n",
    "    assert(len(sent_word)==len(marks))\n",
    "    marked_sent_words.append(marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915301 1915301\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "all_words = []\n",
    "all_marks = []\n",
    "for words, marks in zip(sent_words, marked_sent_words):\n",
    "    all_words += words\n",
    "    all_marks += marks\n",
    "all_words = ''.join(all_words)\n",
    "all_marks = ''.join(all_marks)\n",
    "print(len(all_words), len(all_marks))\n",
    "status = ['b', 'm', 'e', 's']\n",
    "word_marks = {s: [] for s in status}\n",
    "for mark, word in zip(all_marks, all_words):\n",
    "    word_marks[mark].append(word)\n",
    "for s in word_marks:\n",
    "    l = len(word_marks[s])\n",
    "    word_marks[s] = dict(collections.Counter(word_marks[s]))\n",
    "    word_marks[s] = {k: np.log(word_marks[s][k] / l) for k in word_marks[s]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('./word_marks_count.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(json.dumps(word_marks))\n",
    "import json\n",
    "with open('./word_marks_count.json', 'r', encoding='utf-8') as f:\n",
    "    word_marks = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word_cutter():\n",
    "    def __init__(self):\n",
    "        if os.path.exists('../1998-01-2003版-带音.txt'):\n",
    "            self.sents = []\n",
    "            self.sent_words = []\n",
    "            with open('../1998-01-2003版-带音.txt', 'r', encoding='gbk') as f:\n",
    "                for line in f.readlines():\n",
    "                    sent = ''\n",
    "                    sent_word = []\n",
    "                    words = line.strip().split('  ')[1:]\n",
    "                    for word in words:\n",
    "                        sent += (word.split('/')[0].strip() + ' ')\n",
    "                        sent_word.append(word.split('/')[0].strip())\n",
    "                    self.sents.append(sent)\n",
    "                    self.sent_words.append(sent_word)\n",
    "            self.emit_mat = self.get_emit_mat()\n",
    "            self.log_init_status, self.log_trans_mat = self.get_init_status_and_trans_mat()\n",
    "                    \n",
    "    def get_emit_mat(self):\n",
    "        if os.path.exists('./word_marks_count.json'):\n",
    "            print('emit exits')\n",
    "            with open('./word_marks_count.json', 'r', encoding='utf-8') as f:\n",
    "                di = json.loads(f.read())\n",
    "            emit_mat = {}\n",
    "            for word_mark in di:\n",
    "                for word in di[word_mark]:\n",
    "                    if word in emit_mat:\n",
    "                        emit_mat[word][word_mark] = di[word_mark][word]\n",
    "                    else:\n",
    "                        emit_mat[word] = {'b': 0., 'm': 0., 'e': 0., 's': 0.}\n",
    "            return emit_mat\n",
    "        words_set = []\n",
    "        for sent_word in self.sent_words:\n",
    "            words_set += ''.join(sent_word)\n",
    "        words_set = list(set(words_set))\n",
    "\n",
    "        self.marked_sent_words = []\n",
    "        for sent_word in self.sent_words:\n",
    "            marks = []\n",
    "            for word in sent_word:\n",
    "                if len(word) == 1:\n",
    "                    marks.append('s')\n",
    "                elif len(word) == 2:\n",
    "                    marks.append('be')\n",
    "                elif len(word) > 2:\n",
    "                    marks.append('b'+('m'*(len(word)-2))+'e')\n",
    "            assert(len(sent_word)==len(marks))\n",
    "            self.marked_sent_words.append(marks)\n",
    "\n",
    "        all_words = []\n",
    "        all_marks = []\n",
    "        for words, marks in zip(self.sent_words, self.marked_sent_words):\n",
    "            all_words += words\n",
    "            all_marks += marks\n",
    "        all_words = ''.join(all_words)\n",
    "        all_marks = ''.join(all_marks)\n",
    "        status = ['b', 'm', 'e', 's']\n",
    "        word_marks = {s: [] for s in status}\n",
    "        for mark, word in zip(all_marks, all_words):\n",
    "            word_marks[mark].append(word)\n",
    "        for s in word_marks:\n",
    "            l = len(word_marks[s])\n",
    "            word_marks[s] = dict(collections.Counter(word_marks[s]))\n",
    "            word_marks[s] = {k: np.log(word_marks[s][k] / l) for k in word_marks[s]}\n",
    "\n",
    "        marks_per_word = {}\n",
    "        for word_mark in word_marks:\n",
    "            for word in word_marks[word_mark]:\n",
    "                if word in marks_per_word:\n",
    "                    marks_per_word[word][word_mark] = word_marks[word_mark][word]\n",
    "                else:\n",
    "                    marks_per_word[word] = {'b': 0., 'm': 0., 'e': 0., 's': 0.}\n",
    "        return marks_per_word\n",
    "    \n",
    "    def get_init_status_and_trans_mat(self):\n",
    "        if (os.path.exists('./log_init_status.json') and os.path.exists('./log_trans_mat.json')):\n",
    "            print('init exits')\n",
    "            with open('./log_init_status.json', 'r', encoding='utf-8') as f:\n",
    "                log_init_mat = json.loads(f.read())\n",
    "            with open('./log_trans_mat.json', 'r', encoding='utf-8') as f:\n",
    "                log_trans_mat = json.loads(f.read())\n",
    "            return log_init_mat, log_trans_mat\n",
    "        init_status = np.zeros((4, 1))\n",
    "        trans_mat = np.zeros((4, 4))\n",
    "        status2ind = {'b': 0, 'm': 1, 'e': 2, 's': 3}\n",
    "        temp_count = {'b': 0., 's': 0.}\n",
    "        #初始化初始状态\n",
    "        for m in self.marked_sent_words:\n",
    "            if len(m) == 0:\n",
    "                continue\n",
    "            if m[0][0] == 'b':\n",
    "                temp_count['b'] += 1.\n",
    "            elif m[0][0] == 's':\n",
    "                temp_count['s'] += 1.\n",
    "        init_status[0, 0] = temp_count['b'] / (temp_count['b'] + temp_count['s'])\n",
    "        init_status[3, 0] = temp_count['s'] / (temp_count['b'] + temp_count['s'])\n",
    "        #构造转移矩阵\n",
    "        all_marked_seq = ''\n",
    "        for m in self.marked_sent_words:\n",
    "            if len(m) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                all_marked_seq += ''.join(m)\n",
    "        for mark1 ,mark2 in zip(all_marked_seq[:-1], all_marked_seq[1:]):\n",
    "            trans_mat[status2ind[mark1], status2ind[mark2]] += 1.\n",
    "        trans_mat /= np.sum(trans_mat)\n",
    "        log_trans_mat = np.log(trans_mat)\n",
    "        log_init_status = np.log(init_status)\n",
    "        return log_init_status, np.array(log_trans_mat)\n",
    "    \n",
    "    def cut_words(self, txt, emit_mat=None, log_init_status=None, log_trans_mat=None):\n",
    "        if emit_mat == None:\n",
    "            emit_mat = self.get_emit_mat()\n",
    "        if log_init_status == None or log_trans_mat == None:\n",
    "            log_init_status, log_trans_mat = self.get_init_status_and_trans_mat()\n",
    "            print(log_trans_mat)\n",
    "        log_trans_mat = np.array(log_trans_mat)\n",
    "        print(log_init_status)\n",
    "        weights = np.ones((4, len(txt))) * -9999.\n",
    "        status = ['b', 'm', 'e', 's']\n",
    "        weights[:, 0] = np.squeeze(log_init_status) + np.array(list(emit_mat[txt[0]].values())).astype(np.float64)\n",
    "        #初始化第一个字的向量\n",
    "        path = -np.ones((4, len(txt)))\n",
    "        for i, word in enumerate(txt[:-1]):\n",
    "            ind = i + 1\n",
    "            for j in range(4):\n",
    "                for k in range(4):\n",
    "                    temp = weights[k, ind-1] + log_trans_mat[k, j] + float(emit_mat[txt[ind]][status[j]])\n",
    "                    if temp > weights[j, ind]:\n",
    "                        weights[j, ind] = temp\n",
    "                        path[j, ind] = k\n",
    "        endE, endS = weights[2, weights.shape[1]-1], weights[3, weights.shape[1]-1]\n",
    "        inx = 0\n",
    "        if endE > endS:\n",
    "            inx = 2\n",
    "        else:\n",
    "            inx = 3\n",
    "        result = []\n",
    "        for i in reversed(range(len(txt))):\n",
    "            result.append(inx)\n",
    "            inx = path[inx, i].astype(int)\n",
    "        \n",
    "        marks = reversed(result)\n",
    "        cis = []\n",
    "        temp = ''\n",
    "        for mark, letter in zip(marks, txt):\n",
    "            if (mark == 0) or (mark == 1):\n",
    "                temp += letter\n",
    "            elif mark == 2:\n",
    "                temp += letter\n",
    "                cis.append(temp)\n",
    "                temp = ''\n",
    "            elif mark == 3:\n",
    "                temp = ''\n",
    "                cis.append(letter)\n",
    "        return ' '.join(cis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emit exits\n",
      "init exits\n"
     ]
    }
   ],
   "source": [
    "txt = '年来贵州省各级党委和政府把扶贫开发工作作为农村中心任务来抓取得很大成绩贫困人口大量减少贫困状况明显改善'\n",
    "cutter = Word_cutter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emit exits\n",
      "init exits\n",
      "[[-inf, -0.51082562376599, -0.916290731874155, -inf], [-inf, -0.33344856811948514, -1.2603623820268226, -inf], [-0.5897149736854513, -inf, -inf, -0.8085250474669937], [-0.7211965654669841, -inf, -inf, -0.6658631448798212]]\n",
      "[-0.26268660809250016, -inf, -inf, -1.4652633398537678]\n"
     ]
    }
   ],
   "source": [
    "output = cutter.cut_words(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emit exits\n",
      "init exits\n",
      "[[-inf, -0.51082562376599, -0.916290731874155, -inf], [-inf, -0.33344856811948514, -1.2603623820268226, -inf], [-0.5897149736854513, -inf, -inf, -0.8085250474669937], [-0.7211965654669841, -inf, -inf, -0.6658631448798212]]\n",
      "[-0.26268660809250016, -inf, -inf, -1.4652633398537678]\n",
      "今天 天气 真好 啊 <class 'str'>\n",
      "emit exits\n",
      "init exits\n",
      "[[-inf, -0.51082562376599, -0.916290731874155, -inf], [-inf, -0.33344856811948514, -1.2603623820268226, -inf], [-0.5897149736854513, -inf, -inf, -0.8085250474669937], [-0.7211965654669841, -inf, -inf, -0.6658631448798212]]\n",
      "[-0.26268660809250016, -inf, -inf, -1.4652633398537678]\n",
      "新一 届中 共中 央 政治局 常委 将在 人民 大会 堂同 中外 记者 见 面， 欢迎 各位 记者 参加 <class 'str'>\n",
      "emit exits\n",
      "init exits\n",
      "[[-inf, -0.51082562376599, -0.916290731874155, -inf], [-inf, -0.33344856811948514, -1.2603623820268226, -inf], [-0.5897149736854513, -inf, -inf, -0.8085250474669937], [-0.7211965654669841, -inf, -inf, -0.6658631448798212]]\n",
      "[-0.26268660809250016, -inf, -inf, -1.4652633398537678]\n",
      "从梁 家河 一路 走来 的 四十 多年 间， 习近 平始 终以 百姓 之心 为 心。 执政 为民 是他 首要 的政 治品 格， 不变 的 理想 情怀 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import tkinter.messagebox\n",
    "\n",
    "def getPoetry():\n",
    "    txt = title_in.get()\n",
    "    output = ''.join([item[0] for item in cutter.cut_words(txt)])\n",
    "    print(output, type(output))\n",
    "    t.delete('1.0', '2.0')\n",
    "    t.insert('1.0', output)\n",
    "\n",
    "\n",
    "# 庆春， 腊梅香， 清朝， 菩萨， 浣溪沙\n",
    "win = tk.Tk()\n",
    "var = tk.StringVar()\n",
    "var.set(\"\")\n",
    "title_in = tk.Entry(win, textvariable  = var, width=45)\n",
    "title_in.pack()\n",
    "t = tk.Text(win, )\n",
    "\n",
    "t.pack()\n",
    "\n",
    "button_get = tk.Button(win, text=\"开始分词\", command=getPoetry)\n",
    "button_get.pack()\n",
    "win.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
